{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "fK7ZG0dje75w"
   },
   "outputs": [],
   "source": [
    "#@title ALL helper functions and algorithms\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Common policies\n",
    "epsilon = 0.2\n",
    "\n",
    "def _random_argmax(x):\n",
    "    return np.random.choice(np.where(x == np.max(x))[0])\n",
    "\n",
    "def uniformly_random_policy(Q):\n",
    "    pi = np.ones_like(Q) / Q.shape[1]\n",
    "    return pi\n",
    "\n",
    "def greedy_policy(Q):\n",
    "    pi = np.zeros_like(Q)\n",
    "    a_star = np.array([_random_argmax(Q[s]) for s in range(len(Q))])\n",
    "    for s, a in enumerate(a_star):\n",
    "        pi[s, a] = 1\n",
    "    return pi\n",
    "\n",
    "def soft_greedy_policy(Q):\n",
    "    pi = np.zeros_like(Q)\n",
    "    for s in range(len(Q)):\n",
    "        pi[s, np.where(np.isclose(Q[s], np.max(Q[s])))[0]] = 1\n",
    "    return pi / pi.sum(axis=1, keepdims=True)\n",
    "\n",
    "def epsilon_greedy_policy(Q):\n",
    "    #  ε : explore\n",
    "    # 1-ε: exploit\n",
    "    pi = np.ones_like(Q) * epsilon / (Q.shape[1])\n",
    "    a_star = np.array([_random_argmax(Q[s]) for s in range(len(Q))])\n",
    "    for s, a in enumerate(a_star):\n",
    "        pi[s, a] = 1 - epsilon + epsilon / (Q.shape[1])\n",
    "    return pi\n",
    "\n",
    "temperature = 1\n",
    "def boltzmann_softmax_policy(Q):\n",
    "    H = np.exp(Q/temperature)\n",
    "    return H / np.sum(H, axis=1, keepdims=True)\n",
    "\n",
    "# Policies induced by near-equivalent actions\n",
    "zeta = None\n",
    "\n",
    "def zeta_optimal_stochastic_policy(Q, baseline=None):\n",
    "    # assigns equal probability to top near-equivalent actions\n",
    "    baseline = baseline if baseline is not None else Q\n",
    "    pi = np.zeros_like(Q)\n",
    "    Q_cutoff = (1-zeta-tol) * baseline.max(axis=1, keepdims=True)\n",
    "    pi[Q >= Q_cutoff] = 1\n",
    "    pi = pi / pi.sum(axis=1, keepdims=True)\n",
    "    return pi\n",
    "\n",
    "def zeta_optimal_worst_case_policy(Q, baseline=None):\n",
    "    # always choose the worst among the top near-equivalent actions\n",
    "    baseline = baseline if baseline is not None else Q\n",
    "    pi = np.zeros_like(Q)\n",
    "    Q_cutoff = (1-zeta-tol) * baseline.max(axis=1, keepdims=True)\n",
    "    pi[Q >= Q_cutoff] = 1\n",
    "    a_star_hat = np.ma.masked_array(Q, 1-pi).min(axis=1, keepdims=True).data\n",
    "    pi[Q != a_star_hat] = 0\n",
    "    pi = pi / pi.sum(axis=1, keepdims=True)\n",
    "    return pi\n",
    "\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_Q_learning_curves(Qs, Q_star=None):\n",
    "    def flip(items, ncol):\n",
    "        return itertools.chain(*[items[i::ncol] for i in range(ncol)])\n",
    "    \n",
    "    _, nS, nA = Qs.shape\n",
    "    Q_history = Qs.reshape((len(Qs), -1))\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            idx = (nA*s+a)\n",
    "            plt.plot(Q_history[:,idx], \n",
    "                     color=(list(mcolors.TABLEAU_COLORS)*100)[s], \n",
    "                     alpha=1.0-(a//2)*0.4,\n",
    "                     ls=[':', '--', '-.', '-'][a], \n",
    "                     label='$Q(s_{{{}}}, a_{{{}}})$'.format(s,a))\n",
    "    plt.xlabel('num of transitions/updates')\n",
    "    plt.ylabel('Q-value')\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(flip(handles, nA), flip(labels, nA), bbox_to_anchor=(1.04,1), loc=\"upper left\", ncol=nA)\n",
    "    \n",
    "    if Q_star is not None:\n",
    "        global zeta\n",
    "        V_star = Q_star.max(axis=1)\n",
    "        Q_cutoff = (1 - zeta) * V_star\n",
    "        for s, v in enumerate(Q_cutoff):\n",
    "            plt.axhline(v, alpha=0.6, lw=0.75,\n",
    "                        c=(list(mcolors.TABLEAU_COLORS)*100)[s])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_policy(Q, Q_star=None, nS=7, S_terminal=[0,6], directions=None):\n",
    "    if directions is None:\n",
    "        directions = {\n",
    "            0: (-1, 0),\n",
    "            1: (1, 0),\n",
    "            2: (-1, 0.2),\n",
    "            3: (1, 0.2),\n",
    "        }\n",
    "    def get_direction(a):\n",
    "        return directions[a]\n",
    "\n",
    "    grid_size=(1,nS)\n",
    "    nS, nA = Q.shape\n",
    "    data = []\n",
    "    for row in range(grid_size[0]):\n",
    "        for col in range(grid_size[1]):\n",
    "            s = row * grid_size[1] + col\n",
    "            if s in S_terminal:\n",
    "                data.append([col, row, 0, 0, 'none'])\n",
    "            else:\n",
    "                if Q_star is None: # plot deterministic greedy policy\n",
    "                    data.append([col, row, *get_direction(Q[s].argmax()), 'k'])\n",
    "                else: # plot near-optimal policy\n",
    "                    try:\n",
    "                        Q_cutoff = (1-zeta) * Q_star[s].max()\n",
    "                        Q_tilde_s_a = Q[s][Q[s] > Q_cutoff].min()\n",
    "                        for a in range(nA):\n",
    "                            if Q[s,a] == Q[s].max(): # best best action\n",
    "                                data.append([col, row, *get_direction(a), 'k'])\n",
    "                            elif Q[s,a] == Q_tilde_s_a: # worst best action\n",
    "                                data.append([col, row, *get_direction(a), 'r'])\n",
    "                            elif Q[s,a] >= Q_cutoff: # Pi_a = list(np.argwhere(Q[s] > Q_cutoff)[:,0])\n",
    "                                data.append([col, row, *get_direction(a), 'grey'])\n",
    "                            else:\n",
    "                                pass\n",
    "                    except: # fall back on greedy policy\n",
    "                        data.append([col, row, *get_direction(Q[s].argmax()), 'green'])\n",
    "                            \n",
    "    \n",
    "    data = pd.DataFrame(data, columns=['x', 'y', 'u', 'v', 'color'])\n",
    "    plt.figure(figsize=(5,1))\n",
    "    plt.scatter(data['x'], data['y'], s=200, facecolors='none', edgecolors='k')\n",
    "    plt.quiver(data['x'], data['y'], data['u'], data['v'], color=data['color'], width=0.005, scale=2, scale_units='x')\n",
    "    plt.axis('off')\n",
    "    plt.xlim(-0.5, nS+0.5)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def print_policy(pi):\n",
    "    idx = 1\n",
    "    print('action  | ', end='')\n",
    "    for j in range(pi.shape[1]):\n",
    "        print(j+1, end=' ')\n",
    "    print()\n",
    "    print('-'* 18)\n",
    "    for i in pi.tolist():\n",
    "        print('state', idx, '|', end=' ')\n",
    "        idx += 1\n",
    "        for j in i:\n",
    "            if j > 0:\n",
    "                print(1, end=' ')\n",
    "            else:\n",
    "                print('-', end=' ')\n",
    "        print()\n",
    "\n",
    "def display_results_(Q):\n",
    "    Q = Q[1:-1]\n",
    "    V = np.max(Q, axis=1)\n",
    "    print('Q')\n",
    "    print(Q)\n",
    "    print('V')\n",
    "    print(V)\n",
    "    return\n",
    "\n",
    "def calculate_Q_V(Q, Q_star):\n",
    "    # always choose the worst among the top near-equivalent actions\n",
    "    Q_cutoff = (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)\n",
    "    pi = np.zeros_like(Q)\n",
    "    pi[Q >= Q_cutoff] = 1\n",
    "    a_star_hat = np.ma.masked_array(Q, 1-pi).min(axis=1, keepdims=True).data\n",
    "    pi[Q != a_star_hat] = 0\n",
    "    pi = pi / pi.sum(axis=1, keepdims=True)\n",
    "    V = (Q * pi).sum(axis=1)\n",
    "    return Q, V\n",
    "\n",
    "def display_results(Q, Q_star=None):\n",
    "    if Q_star is None:\n",
    "        V = np.max(Q, axis=1)\n",
    "    else:\n",
    "        # always choose the worst among the top near-equivalent actions\n",
    "        Q_cutoff = (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)\n",
    "        pi = np.zeros_like(Q)\n",
    "        pi[Q >= Q_cutoff] = 1\n",
    "        a_star_hat = np.ma.masked_array(Q, 1-pi).min(axis=1, keepdims=True).data\n",
    "        pi[Q != a_star_hat] = 0\n",
    "        pi = pi / pi.sum(axis=1, keepdims=True)\n",
    "        V = (Q * pi).sum(axis=1)\n",
    "    print('Q')\n",
    "    print(Q)\n",
    "    print('V')\n",
    "    print(V)\n",
    "    return Q, V\n",
    "\n",
    "def value_iter(env, gamma=0.9, theta=1e-5):\n",
    "    if not hasattr(env, 'P'):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    P = env.P\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in range(env.nS):\n",
    "            old_v = V[s]\n",
    "            \n",
    "            ## V[s] = max {a} sum {s', r} P[s', r | s, a] * (r + gamma * V[s'])\n",
    "            Q = np.zeros(env.nA)\n",
    "            for a in P[s]:\n",
    "                Q[a] = sum(p * (r + gamma * V[s_]) for p, s_, r, done in P[s][a])\n",
    "            new_v = Q.max()\n",
    "            V[s] = new_v\n",
    "            delta = max(delta, np.abs(new_v - old_v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "def SARSA_eval(env, n_episodes, pi, gamma=1, alpha=0.1):\n",
    "    global epsilon\n",
    "    if not callable(alpha): # step size\n",
    "        alpha_ = alpha\n",
    "        alpha = lambda episode: alpha_\n",
    "    \n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    Gs = []\n",
    "    Qs = [Q.copy()]\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        G = 0\n",
    "        t = 0\n",
    "        np.random.seed(episode)\n",
    "        env.seed(episode)\n",
    "        env.reset()\n",
    "        S = env.s\n",
    "        done = False\n",
    "        A = np.random.choice(env.nA, p=pi[S])\n",
    "        while not done: # S is not a terminal state\n",
    "            S_, R, done, info = env.step(A)\n",
    "            A_ = np.random.choice(env.nA, p=pi[S_])\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * Q[S_,A_] - Q[S,A])\n",
    "            S = S_\n",
    "            A = A_\n",
    "            G = G + (gamma ** t) * R\n",
    "            t = t + 1\n",
    "            Qs.append(Q.copy())\n",
    "        Gs.append(G)\n",
    "    return Q, {'Gs': np.array(Gs), 'Qs': np.array(Qs)}\n",
    "\n",
    "def qlearn(env, n_episodes, behavior_policy, gamma=1, alpha=0.1, Q_init=None, memory=None):\n",
    "    global epsilon\n",
    "    if not callable(alpha): # step size\n",
    "        alpha_ = alpha\n",
    "        alpha = lambda episode: alpha_\n",
    "    \n",
    "    if Q_init is None:\n",
    "        Q = np.zeros((env.nS, env.nA))\n",
    "    else:\n",
    "        Q = Q_init.copy().astype(float)\n",
    "    \n",
    "    if memory is not None:\n",
    "        TD_errors = []\n",
    "        Qs = [Q.copy()]\n",
    "        episode = 0\n",
    "        for S, A, R, S_, done in tqdm(memory):\n",
    "            TD_errors.append(R + gamma * Q[S_].max())\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * Q[S_].max() - Q[S,A])\n",
    "            Qs.append(Q.copy())\n",
    "            if done:\n",
    "                episode += 1\n",
    "        return Q, {'Qs': np.array(Qs), 'TD_errors': np.array(TD_errors)}\n",
    "    \n",
    "    Gs = []\n",
    "    Qs = [Q.copy()]\n",
    "    TD_errors = []\n",
    "    memory_buffer = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        G = 0\n",
    "        t = 0\n",
    "        np.random.seed(episode)\n",
    "        env.seed(episode)\n",
    "        env.reset()\n",
    "        S = env.s\n",
    "        done = False\n",
    "        while not done: # S is not a terminal state\n",
    "            #p = behavior_policy(Q)[S]\n",
    "            p = behavior_policy(Q[[S],:])[0]\n",
    "            A = np.random.choice(env.nA, p=p)\n",
    "            S_, R, done, info = env.step(A)\n",
    "            memory_buffer.append((S, A, R, S_, done))\n",
    "            TD_errors.append(R + gamma * Q[S_].max() - Q[S,A])\n",
    "            \n",
    "            # Perform update\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * Q[S_].max() - Q[S,A])\n",
    "            \n",
    "            S = S_\n",
    "            G = G + (gamma ** t) * R\n",
    "            t = t + 1\n",
    "            Qs.append(Q.copy())\n",
    "        Gs.append(G)\n",
    "    \n",
    "    return Q, {\n",
    "        'Gs': np.array(Gs), # cumulative reward for each episode\n",
    "        'Qs': np.array(Qs), # history of all Q-values per update\n",
    "        'TD_errors': np.array(TD_errors), # temporal difference error for each update\n",
    "        'memory': memory_buffer, # all trajectories/experience, tuples of (s,a,r,s', done)\n",
    "    }\n",
    "\n",
    "def expected_SARSA(env, n_episodes, behavior_policy, target_policy, gamma=1, alpha=0.1):\n",
    "    global epsilon\n",
    "    if not callable(alpha): # step size\n",
    "        alpha_ = alpha\n",
    "        alpha = lambda episode: alpha_\n",
    "    \n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    Gs = []\n",
    "    Qs = [Q.copy()]\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        G = 0\n",
    "        t = 0\n",
    "        np.random.seed(episode)\n",
    "        env.seed(episode)\n",
    "        env.reset()\n",
    "        S = env.s\n",
    "        done = False\n",
    "        while not done: # S is not a terminal state\n",
    "            A = np.random.choice(env.nA, p=behavior_policy(Q)[S])\n",
    "            S_, R, done, info = env.step(A)\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * (target_policy(Q)[S_] @ Q[S_]) - Q[S,A])\n",
    "            S = S_\n",
    "            G = G + (gamma ** t) * R\n",
    "            t = t + 1\n",
    "            Qs.append(Q.copy())\n",
    "        Gs.append(G)\n",
    "    return Q, {'Gs': np.array(Gs), 'Qs': np.array(Qs)}\n",
    "\n",
    "def TD_conservative(env, n_episodes, behavior_policy, target_policy=None, \n",
    "                    gamma=1, alpha=0.1, Q_init=None, Q_star=None, memory=None):\n",
    "    if Q_star is None:\n",
    "        assert False\n",
    "    if Q_init is None:\n",
    "        Q_init = np.zeros_like(Q_star)\n",
    "    \n",
    "    global epsilon\n",
    "    if not callable(alpha): # step size\n",
    "        alpha_ = alpha\n",
    "        alpha = lambda episode: alpha_\n",
    "    \n",
    "    Q = Q_init.copy().astype(float)\n",
    "    \n",
    "    if memory is not None:\n",
    "        TD_errors = []\n",
    "        Qs = [Q.copy()]\n",
    "        episode = 0\n",
    "        for S, A, R, S_, done in tqdm(memory):\n",
    "            TD_errors.append(R + gamma * (1-zeta) * Q_star[S_].max())\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * (1-zeta) * Q_star[S_].max() - Q[S,A])\n",
    "            Qs.append(Q.copy())\n",
    "            if done:\n",
    "                episode += 1\n",
    "        return Q, {'Qs': np.array(Qs), 'TD_errors': np.array(TD_errors)}\n",
    "    \n",
    "    Gs = []\n",
    "    Qs = [Q.copy()]\n",
    "    TD_errors = []\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        G = 0\n",
    "        t = 0\n",
    "        np.random.seed(episode)\n",
    "        env.seed(episode)\n",
    "        env.reset()\n",
    "        S = env.s\n",
    "        done = False\n",
    "        while not done: # S is not a terminal state\n",
    "            A = np.random.choice(env.nA, p=behavior_policy(Q)[S])\n",
    "            S_, R, done, info = env.step(A)\n",
    "            TD_errors.append(R + gamma * (1-zeta) * Q_star[S_].max())\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * (1-zeta) * Q_star[S_].max() - Q[S,A])\n",
    "            S = S_\n",
    "            G = G + (gamma ** t) * R\n",
    "            t = t + 1\n",
    "            Qs.append(Q.copy())\n",
    "        Gs.append(G)\n",
    "    return Q, {'Gs': np.array(Gs), 'Qs': np.array(Qs), 'TD_errors': np.array(TD_errors)}\n",
    "\n",
    "def TD_improved(env, n_episodes, behavior_policy, target_policy=None, \n",
    "                gamma=1, alpha=0.1, Q_init=None, Q_star=None, memory=None):\n",
    "    if Q_star is None:\n",
    "        assert False\n",
    "    if Q_init is None:\n",
    "        Q_init = np.zeros_like(Q_star)\n",
    "    \n",
    "    global epsilon\n",
    "    if not callable(alpha): # step size\n",
    "        alpha_ = alpha\n",
    "        alpha = lambda episode: alpha_\n",
    "    \n",
    "    Q = Q_init.copy().astype(float)\n",
    "    \n",
    "    if memory is not None:\n",
    "        TD_errors = []\n",
    "        Qs = [Q.copy()]\n",
    "        episode = 0\n",
    "        for S, A, R, S_, done in tqdm(memory):\n",
    "            Q_cutoff = (1-zeta) * Q_star[S_].max() # lower bound for future return\n",
    "            Pi_S = np.argwhere(Q[S_] > Q_cutoff)\n",
    "            if len(Pi_S) > 0: # improve the lower bound\n",
    "                Q_tilde_s_a = Q[S_][Pi_S].min() # using the worst best action\n",
    "            else:\n",
    "                # Q_tilde_s_a = Q_cutoff # fall back to the lower bound\n",
    "                Q_tilde_s_a = Q[S_].max() # fall back to the greedy action\n",
    "            \n",
    "            TD_errors.append(R + gamma * Q_tilde_s_a - Q[S,A])\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * Q_tilde_s_a - Q[S,A])\n",
    "            Qs.append(Q.copy())\n",
    "            if done:\n",
    "                episode += 1\n",
    "        return Q, {'Qs': np.array(Qs), 'TD_errors': np.array(TD_errors)}\n",
    "    \n",
    "    Gs = []\n",
    "    Qs = [Q.copy()]\n",
    "    TD_errors = []\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        G = 0\n",
    "        t = 0\n",
    "        np.random.seed(episode)\n",
    "        env.seed(episode)\n",
    "        env.reset()\n",
    "        S = env.s\n",
    "        done = False\n",
    "        while not done: # S is not a terminal state\n",
    "            A = np.random.choice(env.nA, p=behavior_policy(Q)[S])\n",
    "            S_, R, done, info = env.step(A)\n",
    "            Q_cutoff = (1-zeta) * Q_star[S_].max() # lower bound for future return\n",
    "            Pi_S = np.argwhere(Q[S_] > Q_cutoff)\n",
    "            if len(Pi_S) > 0: # improve the lower bound\n",
    "                Q_tilde_s_a = Q[S_][Pi_S].min() # using the worst best action\n",
    "            else:\n",
    "                # Q_tilde_s_a = Q_cutoff # fall back to the lower bound\n",
    "                Q_tilde_s_a = Q[S_].max() # fall back to the greedy action\n",
    "            \n",
    "            TD_errors.append(R + gamma * Q_tilde_s_a - Q[S,A])\n",
    "            Q[S,A] = Q[S,A] + alpha(episode) * (R + gamma * Q_tilde_s_a - Q[S,A])\n",
    "            S = S_\n",
    "            G = G + (gamma ** t) * R\n",
    "            t = t + 1\n",
    "            Qs.append(Q.copy())\n",
    "        Gs.append(G)\n",
    "    return Q, {'Gs': np.array(Gs), 'Qs': np.array(Qs), 'TD_errors': np.array(TD_errors)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOJZ9Vlq47fK"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bfSNd0Jry1GE"
   },
   "outputs": [],
   "source": [
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
    "from gym.envs.toy_text import discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpDA7Zcj3g5n"
   },
   "outputs": [],
   "source": [
    "def visualize_policy(policy, ncol=1, terminal=[]):\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "\n",
    "    _actions = ['L', 'D', 'R', 'U']\n",
    "    _actions = ['←', '↓', '→', '↑']\n",
    "    _actions = ['<', 'V', '>', 'A']\n",
    "\n",
    "    for i, pi_a_s in enumerate(policy):\n",
    "        if i in terminal:\n",
    "            print('{:^6}'.format(''), end='|')\n",
    "        else:\n",
    "            idxs = np.where(pi_a_s > 0)[0]\n",
    "            actions_str = ''.join(_actions[idx] for idx in idxs)\n",
    "            print('{:^6}'.format(actions_str), end='|')\n",
    "        if (i+1) % ncol == 0:\n",
    "            print('\\n', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def MC_eval(env, n_episodes, pi, gamma=1, max_steps=1000):\n",
    "    def _MC_generate_episode(env, pi, seed):\n",
    "        np.random.seed(seed)\n",
    "        env.seed(seed)\n",
    "        env.reset()\n",
    "\n",
    "        # Generate an episode\n",
    "        S = env.s = seed % env.nS\n",
    "        done = False\n",
    "        t = 0\n",
    "        transitions = []\n",
    "        while not done and t < max_steps: # S is not a terminal state\n",
    "            A = np.random.choice(env.nA, p=pi[S])\n",
    "            S_, R, done, info = env.step(A)\n",
    "            transitions.append((S,A,R,S_))\n",
    "            S = S_\n",
    "            t = t + 1\n",
    "\n",
    "        # Calculate return for each visited state\n",
    "        trajectory = []\n",
    "        G = 0\n",
    "        for t in reversed(range(len(transitions))):\n",
    "            S, A, R, S_ = transitions[t]\n",
    "            G = gamma * G + R\n",
    "            trajectory.append((S,A,R,S_,G))\n",
    "\n",
    "        return list(reversed(trajectory))\n",
    "    \n",
    "    out = Parallel(n_jobs=64)(delayed(_MC_generate_episode)(env, pi, int(1e5+i)) for i in range(n_episodes))\n",
    "    all_transitions = sum(out, [])\n",
    "    returns_by_state = {\n",
    "        s: [G for (S,A,R,S_,G) in all_transitions if s == S]\n",
    "        for s in range(env.nS)\n",
    "    }\n",
    "    V_pi = np.array([np.mean(Gs) for s, Gs in sorted(returns_by_state.items())])\n",
    "    return V_pi\n",
    "\n",
    "\n",
    "def np_exclude(x, exclude):\n",
    "    return x[~np.isin(np.arange(len(x)), exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucHFSrUFs8Tc"
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "import sys\n",
    "from gym import utils\n",
    "class FrozenLakeEnv_slip_rand_reward(discrete.DiscreteEnv):\n",
    "    def __init__(self, desc=None, map_name=\"4x4\", slip_prob=0.0, reward_modifier=None):\n",
    "        if reward_modifier is None:\n",
    "            reward_modifier = [0., 0., 0., 0.]\n",
    "        \n",
    "        if desc is None and map_name is None:\n",
    "            desc = generate_random_map()\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "        \n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "        S_terminal = self.S_terminal = []\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "\n",
    "        def inc(row, col, a):\n",
    "            if a == LEFT:\n",
    "                col = max(col-1,0)\n",
    "            elif a == DOWN:\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a == RIGHT:\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a == UP:\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                rand_rew = np.random.permutation(reward_modifier)\n",
    "                s = to_s(row, col)\n",
    "                if desc[row, col] in b'GH': # terminal absorbing states\n",
    "                    S_terminal.append(s)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'GH': # terminal absorbing states\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                            newrow, newcol = inc(row, col, b)\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            rew = float(newletter == b'G') + rand_rew[a] # reward mod determined by intended action\n",
    "                            if b != a:\n",
    "                                li.append((slip_prob/2.0, newstate, rew, done))\n",
    "                            else:\n",
    "                                li.append((1.0-slip_prob, newstate, rew, done))\n",
    "        \n",
    "        super().__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            with closing(outfile):\n",
    "                return outfile.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x4 deterministic rand_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "env = FrozenLakeEnv_slip_rand_reward(\n",
    "    map_name=\"4x4\", \n",
    "    slip_prob=0.0,\n",
    "    reward_modifier = [0.00, 0.01, 0.02, 0.03],\n",
    ")\n",
    "nS, nA = env.nS, env.nA\n",
    "S_terminal = env.S_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/frozenlake_4x4_rrew.Q.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    Q_star = tmp['Qs'][-1] # Q table after the final update\n",
    "    V_star = Q_star.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "zeta: 0.0\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): -8.881784197001252e-16\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.01\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): -8.881784197001252e-16\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.02\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.03\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.04\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.05\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.06\n",
      "Policy size : 13\n",
      "Policy suboptimality (estimated): 0.057785765054636506\n",
      "Policy suboptimality (MC): 0.05778576505463262\n",
      "  V>  |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.07\n",
      "Policy size : 13\n",
      "Policy suboptimality (estimated): 0.057785765054636506\n",
      "Policy suboptimality (MC): 0.05778576505463262\n",
      "  V>  |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.08\n",
      "Policy size : 13\n",
      "Policy suboptimality (estimated): 0.06934762082714141\n",
      "Policy suboptimality (MC): 0.05778576505463262\n",
      "  V>  |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.09\n",
      "Policy size : 15\n",
      "Policy suboptimality (estimated): 0.08991134046185911\n",
      "Policy suboptimality (MC): 0.6213093883172053\n",
      "  V>  |  >   |  V   |  <>  |\n",
      "  V   |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.1\n",
      "Policy size : 14\n",
      "Policy suboptimality (estimated): 0.09983706692275551\n",
      "Policy suboptimality (MC): 0.7599668220807404\n",
      "  V>  |  >   |  VA  |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.11\n",
      "Policy size : 14\n",
      "Policy suboptimality (estimated): 0.10933778005520156\n",
      "Policy suboptimality (MC): 0.7599668220807404\n",
      "  V>  |  >   |  VA  |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.12\n",
      "Policy size : 15\n",
      "Policy suboptimality (estimated): 0.11694806723619633\n",
      "Policy suboptimality (MC): 0.7599668220807404\n",
      "  V>  |  >   |  VA  |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  V>  |  >   |      |\n"
     ]
    }
   ],
   "source": [
    "### Greedy mostly converged\n",
    "gamma = 0.9\n",
    "zeta_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12]\n",
    "Q_pi_ALL = {}\n",
    "for zeta in zeta_range:\n",
    "    print('='*30)\n",
    "    print('zeta:', zeta)\n",
    "    with open('output/frozenlake_4x4_rrew.greedy.zeta={}.pkl'.format(zeta), 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "        Q = metadata['Qs'][-1] # Q table after the final update\n",
    "    \n",
    "    Q_pi_ALL[zeta] = Q\n",
    "    Q_pi, V_pi = calculate_Q_V(Q, Q_star)\n",
    "    df_pi = pd.DataFrame(Q >= (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)).astype(int)\n",
    "    df_pi.index.name = 'S'\n",
    "    df_pi.columns.name = 'A'\n",
    "    print('Policy size :', df_pi.drop(S_terminal).sum().sum()) # policy size for non-terminal states\n",
    "    print(\n",
    "        'Policy suboptimality (estimated):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    \n",
    "    pi = zeta_optimal_worst_case_policy(Q, Q_star)\n",
    "    V_pi = MC_eval(env, 1000, pi, gamma=gamma, max_steps=1000)\n",
    "    print(\n",
    "        'Policy suboptimality (MC):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    visualize_policy(zeta_optimal_stochastic_policy(Q, Q_star), 4, S_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "zeta: 0.0\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): -8.881784197001252e-16\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.01\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): -8.881784197001252e-16\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.02\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.03\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.04\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.05\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.011463945890175409\n",
      "Policy suboptimality (MC): 0.011463945890173521\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.06\n",
      "Policy size : 14\n",
      "Policy suboptimality (estimated): 0.058133478842754216\n",
      "Policy suboptimality (MC): 0.5851027753316864\n",
      "  V>  |  >   |  V   |  <>  |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.07\n",
      "Policy size : 16\n",
      "Policy suboptimality (estimated): 0.06832101372756205\n",
      "Policy suboptimality (MC): 0.6860612460401264\n",
      "  V>  |  >   |  V   |  <>  |\n",
      "  V   |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  V>  |  >   |      |\n",
      "==============================\n",
      "zeta: 0.08\n",
      "Policy size : 21\n",
      "Policy suboptimality (estimated): 0.07452456468849966\n",
      "Policy suboptimality (MC): 0.7475384360630257\n",
      " V>A  |  >A  |  VA  | <>A  |\n",
      "  <V  |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  V>  |  >   |      |\n",
      "==============================\n",
      "zeta: 0.09\n",
      "Policy size : 22\n",
      "Policy suboptimality (estimated): 0.08518313716547843\n",
      "Policy suboptimality (MC): 0.8531648893098849\n",
      " <V>A |  >A  |  VA  | <>A  |\n",
      "  <V  |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  V>  |  >   |      |\n",
      "==============================\n",
      "zeta: 0.1\n",
      "Policy size : 23\n",
      "Policy suboptimality (estimated): 0.09029126213592709\n",
      "Policy suboptimality (MC): 0.9037875040054868\n",
      " <V>A |  >A  |  VA  | <>A  |\n",
      "  <V  |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  V>  |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.11\n",
      "Policy size : 23\n",
      "Policy suboptimality (estimated): 0.09029126213592709\n",
      "Policy suboptimality (MC): 0.9037875040054868\n",
      " <V>A |  >A  |  VA  | <>A  |\n",
      "  <V  |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  V>  |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.12\n",
      "Policy size : 24\n",
      "Policy suboptimality (estimated): 0.11045360980123275\n",
      "Policy suboptimality (MC): 0.9037875040054868\n",
      " <V>A |  >A  |  VA  | <>A  |\n",
      " <VA  |      |  V   |      |\n",
      "  <>  |  V>  |  V   |      |\n",
      "      |  V>  |  V>  |      |\n"
     ]
    }
   ],
   "source": [
    "### Baseline: threshold based on Q_star\n",
    "gamma = 0.9\n",
    "zeta_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12]\n",
    "for zeta in zeta_range:\n",
    "    print('='*30)\n",
    "    print('zeta:', zeta)\n",
    "    Q = Q_star.copy()\n",
    "    Q_pi, V_pi = calculate_Q_V(Q, Q_star)\n",
    "    df_pi = pd.DataFrame(Q >= (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)).astype(int)\n",
    "    df_pi.index.name = 'S'\n",
    "    df_pi.columns.name = 'A'\n",
    "    print('Policy size :', df_pi.drop(S_terminal).sum().sum()) # policy size for non-terminal states\n",
    "    print(\n",
    "        'Policy suboptimality (estimated):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    \n",
    "    pi = zeta_optimal_worst_case_policy(Q, Q_star)\n",
    "    V_pi = MC_eval(env, 1000, pi, gamma=gamma, max_steps=1000)\n",
    "    print(\n",
    "        'Policy suboptimality (MC):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    visualize_policy(zeta_optimal_stochastic_policy(Q, Q_star), 4, S_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8x8 deterministic rand_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "env = FrozenLakeEnv_slip_rand_reward(\n",
    "    map_name=\"8x8\", \n",
    "    slip_prob=0.0,\n",
    "    reward_modifier = [0.000, 0.001, 0.002, 0.003],\n",
    ")\n",
    "nS, nA = env.nS, env.nA\n",
    "S_terminal = env.S_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/frozenlake_8x8_rrew.Q.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    Q_star = tmp['Qs'][-1] # Q table after the final update\n",
    "    V_star = Q_star.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "zeta: 0.0\n",
      "Policy size : 53\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.0\n",
      "  V   |  V   |  V   |  >   |  V   |  >   |  V   |  V   |\n",
      "  V   |  >   |  >   |  >   |  V   |  V   |  >   |  V   |\n",
      "  >   |  >   |  V   |      |  V   |  >   |  V   |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  >   |  V   |\n",
      "  >   |  >   |  A   |      |  V   |  V   |  >   |  V   |\n",
      "  A   |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.01\n",
      "Policy size : 66\n",
      "Policy suboptimality (estimated): 0.009258404059738101\n",
      "Policy suboptimality (MC): 0.009258404059737213\n",
      "  V   |  V   |  V>  |  >   |  V   |  V>  |  V>  |  V   |\n",
      "  V   |  V>  |  >   |  >   |  V   |  V>  |  V>  |  V   |\n",
      "  >   |  >   |  V   |      |  V   |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.02\n",
      "Policy size : 72\n",
      "Policy suboptimality (estimated): 0.019926949962416063\n",
      "Policy suboptimality (MC): 0.019926949962415286\n",
      "  V   |  V   |  V   |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V   |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.03\n",
      "Policy size : 75\n",
      "Policy suboptimality (estimated): 0.022422917528537423\n",
      "Policy suboptimality (MC): 0.0224229175285362\n",
      "  V>  |  V   |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.04\n",
      "Policy size : 75\n",
      "Policy suboptimality (estimated): 0.03427334509518298\n",
      "Policy suboptimality (MC): 0.034273345095182095\n",
      "  V   |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.05\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.04083684295596979\n",
      "Policy suboptimality (MC): 0.0408368429559689\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.06\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.04083684295596979\n",
      "Policy suboptimality (MC): 0.0408368429559689\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.07\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.04083684295596979\n",
      "Policy suboptimality (MC): 0.0408368429559689\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.08\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.04083684295596979\n",
      "Policy suboptimality (MC): 0.0408368429559689\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.09\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.04083684295596979\n",
      "Policy suboptimality (MC): 0.0408368429559689\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.1\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.09942028985507245\n",
      "Policy suboptimality (MC): 0.9801813377423819\n",
      "  V   |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V>  |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.11\n",
      "Policy size : 75\n",
      "Policy suboptimality (estimated): 0.10828492943634416\n",
      "Policy suboptimality (MC): 1.0\n",
      "  V   |  V   |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V   |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V>  |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.12\n",
      "Policy size : 74\n",
      "Policy suboptimality (estimated): 0.11769620040250639\n",
      "Policy suboptimality (MC): 0.9801813596101518\n",
      "  V>  |  V>  |  V>  |  V>  |  V   |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V   |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V   |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V>  |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n"
     ]
    }
   ],
   "source": [
    "### Greedy mostly converged\n",
    "gamma = 0.9\n",
    "zeta_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12]\n",
    "Q_pi_ALL = {}\n",
    "for zeta in zeta_range:\n",
    "    print('='*30)\n",
    "    print('zeta:', zeta)\n",
    "    with open('output/frozenlake_8x8_rrew.greedy.zeta={}.pkl'.format(zeta), 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "        Q = metadata['Qs'][-1] # Q table after the final update\n",
    "    \n",
    "    Q_pi_ALL[zeta] = Q\n",
    "    Q_pi, V_pi = calculate_Q_V(Q, Q_star)\n",
    "    df_pi = pd.DataFrame(Q >= (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)).astype(int)\n",
    "    df_pi.index.name = 'S'\n",
    "    df_pi.columns.name = 'A'\n",
    "    print('Policy size :', df_pi.drop(S_terminal).sum().sum()) # policy size for non-terminal states\n",
    "    print(\n",
    "        'Policy suboptimality (estimated):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    \n",
    "    pi = zeta_optimal_worst_case_policy(Q, Q_star)\n",
    "    V_pi = MC_eval(env, 1000, pi, gamma=gamma, max_steps=1000)\n",
    "    print(\n",
    "        'Policy suboptimality (MC):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    visualize_policy(zeta_optimal_stochastic_policy(Q, Q_star), 8, S_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "zeta: 0.0\n",
      "Policy size : 53\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.0\n",
      "  V   |  V   |  V   |  >   |  V   |  >   |  V   |  V   |\n",
      "  V   |  >   |  >   |  >   |  V   |  V   |  >   |  V   |\n",
      "  >   |  >   |  V   |      |  V   |  >   |  V   |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  >   |  V   |\n",
      "  >   |  >   |  A   |      |  V   |  V   |  >   |  V   |\n",
      "  A   |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.01\n",
      "Policy size : 74\n",
      "Policy suboptimality (estimated): 0.008637345131774277\n",
      "Policy suboptimality (MC): 0.0224229175285362\n",
      "  V>  |  V   |  V>  |  V>  |  V   |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.02\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.03\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.04\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.05\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.06\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.07\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.08\n",
      "Policy size : 76\n",
      "Policy suboptimality (estimated): 0.013003352493702436\n",
      "Policy suboptimality (MC): 0.03946884895315028\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.09\n",
      "Policy size : 77\n",
      "Policy suboptimality (estimated): 0.08995451655250397\n",
      "Policy suboptimality (MC): 0.900449259035313\n",
      "  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V>  |  V   |\n",
      " <V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V   |\n",
      "  V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V   |\n",
      "  >   |  >   |  >   |  >   |  V   |      |  V>  |  V   |\n",
      "  >A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V   |\n",
      "  VA  |      |      |  >   |  >   |  V   |      |  V   |\n",
      "  V   |      |  >   |  A   |      |  V   |      |  V   |\n",
      "  >   |  >   |  A   |      |  >   |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.1\n",
      "Policy size : 103\n",
      "Policy suboptimality (estimated): 0.1000000000000002\n",
      "Policy suboptimality (MC): 1.0\n",
      " <V>A | V>A  | V>A  | V>A  | V>A  | V>A  | V>A  | V>A  |\n",
      " <V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V>  |\n",
      " <V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V>  |\n",
      "  <>  |  >   |  >   |  >   |  V   |      |  V>  |  V>  |\n",
      " <>A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V>  |\n",
      " <VA  |      |      |  >   |  >   |  V   |      |  V>  |\n",
      "  <V  |      |  >   |  A   |      |  V   |      |  V>  |\n",
      " <V>  |  V>  |  VA  |      |  V>  |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.11\n",
      "Policy size : 105\n",
      "Policy suboptimality (estimated): 0.1000000994033311\n",
      "Policy suboptimality (MC): 1.0\n",
      " <V>A | V>A  | V>A  | V>A  | V>A  | V>A  | V>A  | V>A  |\n",
      " <V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V>  |\n",
      " <V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V>  |\n",
      "  <>  |  >   |  >   |  >   |  V   |      |  V>  |  V>  |\n",
      " <>A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V>  |\n",
      " <VA  |      |      |  >   |  >   |  V   |      |  V>  |\n",
      "  <V  |      |  >   |  A   |      |  V   |      |  V>  |\n",
      " <V>  |  V>  |  VA  |      |  V>  |  V>  |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.12\n",
      "Policy size : 105\n",
      "Policy suboptimality (estimated): 0.1000000994033311\n",
      "Policy suboptimality (MC): 1.0\n",
      " <V>A | V>A  | V>A  | V>A  | V>A  | V>A  | V>A  | V>A  |\n",
      " <V>  |  V>  |  V>  |  >   |  V>  |  V>  |  V>  |  V>  |\n",
      " <V>  |  V>  |  V   |      |  V>  |  >   |  V>  |  V>  |\n",
      "  <>  |  >   |  >   |  >   |  V   |      |  V>  |  V>  |\n",
      " <>A  |  >A  |  A   |      |  V>  |  V>  |  >   |  V>  |\n",
      " <VA  |      |      |  >   |  >   |  V   |      |  V>  |\n",
      "  <V  |      |  >   |  A   |      |  V   |      |  V>  |\n",
      " <V>  |  V>  |  VA  |      |  V>  |  V>  |  V>  |      |\n"
     ]
    }
   ],
   "source": [
    "### Baseline threshold based on Q_star\n",
    "gamma = 0.9\n",
    "zeta_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12]\n",
    "for zeta in zeta_range:\n",
    "    print('='*30)\n",
    "    print('zeta:', zeta)\n",
    "    Q = Q_star.copy()\n",
    "    Q_pi, V_pi = calculate_Q_V(Q, Q_star)\n",
    "    df_pi = pd.DataFrame(Q >= (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)).astype(int)\n",
    "    df_pi.index.name = 'S'\n",
    "    df_pi.columns.name = 'A'\n",
    "    print('Policy size :', df_pi.drop(S_terminal).sum().sum()) # policy size for non-terminal states\n",
    "    print(\n",
    "        'Policy suboptimality (estimated):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    \n",
    "    pi = zeta_optimal_worst_case_policy(Q, Q_star)\n",
    "    V_pi = MC_eval(env, 1000, pi, gamma=gamma, max_steps=1000)\n",
    "    print(\n",
    "        'Policy suboptimality (MC):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    visualize_policy(zeta_optimal_stochastic_policy(Q, Q_star), 8, S_terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x4 stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "env = FrozenLakeEnv_slip_rand_reward(\n",
    "    map_name=\"4x4\", \n",
    "    slip_prob=0.1,\n",
    "    reward_modifier = [0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "nS, nA = env.nS, env.nA\n",
    "S_terminal = env.S_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/frozenlake_4x4_slip.Q.pkl', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    Q_star = tmp['Qs'][-1] # Q table after the final update\n",
    "    V_star = Q_star.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "zeta: 0.0\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.01\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.02\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 7.771561172376096e-16\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.03\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 3.055333763768431e-13\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.04\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 3.801283066096062e-09\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.05\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 4.184782208538351e-07\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.06\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 5.691559533693713e-05\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.07\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.030131752228069852\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.08\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.0729658876256023\n",
      "Policy suboptimality (MC): 0.08055319760117419\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.09\n",
      "Policy size : 13\n",
      "Policy suboptimality (estimated): 0.08560014129425253\n",
      "Policy suboptimality (MC): 0.09188073841049493\n",
      "  V>  |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.1\n",
      "Policy size : 15\n",
      "Policy suboptimality (estimated): 0.10047249198152486\n",
      "Policy suboptimality (MC): 0.8917800430749211\n",
      "  V>  |  >   |  V   |  <A  |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.11\n",
      "Policy size : 15\n",
      "Policy suboptimality (estimated): 0.11000385761725806\n",
      "Policy suboptimality (MC): 0.8917800430749211\n",
      "  V>  |  >   |  V   |  <A  |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.12\n",
      "Policy size : 15\n",
      "Policy suboptimality (estimated): 0.1200020209736099\n",
      "Policy suboptimality (MC): 0.8917800430749211\n",
      "  V>  |  >   |  V   |  <A  |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  V>  |      |\n"
     ]
    }
   ],
   "source": [
    "### Greedy mostly converged\n",
    "gamma = 0.9\n",
    "zeta_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12]\n",
    "Q_pi_ALL = {}\n",
    "for zeta in zeta_range:\n",
    "    print('='*30)\n",
    "    print('zeta:', zeta)\n",
    "    with open('output/frozenlake_4x4_slip.greedy.zeta={}.pkl'.format(zeta), 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "        Q = metadata['Qs'][-1] # Q table after the final update\n",
    "    \n",
    "    Q_pi_ALL[zeta] = Q\n",
    "    Q_pi, V_pi = calculate_Q_V(Q, Q_star)\n",
    "    df_pi = pd.DataFrame(Q >= (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)).astype(int)\n",
    "    df_pi.index.name = 'S'\n",
    "    df_pi.columns.name = 'A'\n",
    "    print('Policy size :', df_pi.drop(S_terminal).sum().sum()) # policy size for non-terminal states\n",
    "    print(\n",
    "        'Policy suboptimality (estimated):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    \n",
    "    pi = zeta_optimal_worst_case_policy(Q, Q_star)\n",
    "    V_pi = MC_eval(env, 5000, pi, gamma=gamma, max_steps=1000)\n",
    "    print(\n",
    "        'Policy suboptimality (MC):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    visualize_policy(zeta_optimal_stochastic_policy(Q, Q_star), 4, S_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "zeta: 0.0\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.01\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.02\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.03\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.04\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.05\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.06\n",
      "Policy size : 11\n",
      "Policy suboptimality (estimated): 0.0\n",
      "Policy suboptimality (MC): 0.01815768656425576\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V   |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.07\n",
      "Policy size : 12\n",
      "Policy suboptimality (estimated): 0.07036693225385937\n",
      "Policy suboptimality (MC): 0.08055319760117419\n",
      "  V   |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.08\n",
      "Policy size : 13\n",
      "Policy suboptimality (estimated): 0.07573911107429254\n",
      "Policy suboptimality (MC): 0.09188073841049493\n",
      "  V>  |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.09\n",
      "Policy size : 13\n",
      "Policy suboptimality (estimated): 0.07573911107429254\n",
      "Policy suboptimality (MC): 0.09188073841049493\n",
      "  V>  |  >   |  V   |  <   |\n",
      "  V   |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  >   |      |\n",
      "==============================\n",
      "zeta: 0.1\n",
      "Policy size : 19\n",
      "Policy suboptimality (estimated): 0.099479949472711\n",
      "Policy suboptimality (MC): 0.968512352389125\n",
      " <V>A |  >A  |  V   |  <A  |\n",
      "  <V  |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.11\n",
      "Policy size : 19\n",
      "Policy suboptimality (estimated): 0.099479949472711\n",
      "Policy suboptimality (MC): 0.968512352389125\n",
      " <V>A |  >A  |  V   |  <A  |\n",
      "  <V  |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  V>  |      |\n",
      "==============================\n",
      "zeta: 0.12\n",
      "Policy size : 20\n",
      "Policy suboptimality (estimated): 0.11440970646386228\n",
      "Policy suboptimality (MC): 1.0\n",
      " <V>A |  >A  |  VA  |  <A  |\n",
      "  <V  |      |  V   |      |\n",
      "  >   |  V>  |  V   |      |\n",
      "      |  >   |  V>  |      |\n"
     ]
    }
   ],
   "source": [
    "### Baseline: threshold based on Q_star\n",
    "gamma = 0.9\n",
    "zeta_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12]\n",
    "for zeta in zeta_range:\n",
    "    print('='*30)\n",
    "    print('zeta:', zeta)\n",
    "    Q = Q_star.copy()\n",
    "    Q_pi, V_pi = calculate_Q_V(Q, Q_star)\n",
    "    df_pi = pd.DataFrame(Q >= (1-zeta-tol) * Q_star.max(axis=1, keepdims=True)).astype(int)\n",
    "    df_pi.index.name = 'S'\n",
    "    df_pi.columns.name = 'A'\n",
    "    print('Policy size :', df_pi.drop(S_terminal).sum().sum()) # policy size for non-terminal states\n",
    "    print(\n",
    "        'Policy suboptimality (estimated):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    \n",
    "    pi = zeta_optimal_worst_case_policy(Q, Q_star)\n",
    "    V_pi = MC_eval(env, 5000, pi, gamma=gamma, max_steps=1000)\n",
    "    print(\n",
    "        'Policy suboptimality (MC):', \n",
    "        (1 - np_exclude(V_pi, S_terminal) / np_exclude(V_star, S_terminal)).max()\n",
    "    )\n",
    "    visualize_policy(zeta_optimal_stochastic_policy(Q, Q_star), 4, S_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
